{
    "docs": [
        {
            "location": "/",
            "text": "Tiramisu - Natural language processing through Deep learning\n\n\nWelcome to the documentation of the Tiramisu Project !\n\n\nThe Tiramisu Project is a set of softwares dealing with Natural language processing and Deep learning.\nCurrently, this project contains a \nGutenberg crawler\n, an \nAuthor classification system\n and a \nText Generation software\n. Feel free to use, fork and comment !\n\n\nAny software of this project is expected to be used on \nUbuntu >= 16.04\n. The use of another distribution might lead to some problems.\n\n\nTiramisu is currently compatible with: \nPython3.5\n.\n\n\nSupport for \nPython2.7\n will be added soon !\n\n\n\n\nYou will find on this website all the documentation available for the stable version of this project.\n\n\nIf any of your question remains unanswered, feel free to \ncontact us\n !",
            "title": "Home"
        },
        {
            "location": "/#tiramisu-natural-language-processing-through-deep-learning",
            "text": "",
            "title": "Tiramisu - Natural language processing through Deep learning"
        },
        {
            "location": "/#welcome-to-the-documentation-of-the-tiramisu-project",
            "text": "The Tiramisu Project is a set of softwares dealing with Natural language processing and Deep learning.\nCurrently, this project contains a  Gutenberg crawler , an  Author classification system  and a  Text Generation software . Feel free to use, fork and comment !  Any software of this project is expected to be used on  Ubuntu >= 16.04 . The use of another distribution might lead to some problems.  Tiramisu is currently compatible with:  Python3.5 .  Support for  Python2.7  will be added soon !   You will find on this website all the documentation available for the stable version of this project.  If any of your question remains unanswered, feel free to  contact us  !",
            "title": "Welcome to the documentation of the Tiramisu Project !"
        },
        {
            "location": "/author_recognition_guide/",
            "text": "Getting started with the author recognition software\n\n\nThe dataset used for the following can be found \nhere\n. It contains texts from Balzac, France, Hugo, Maupassant, Proust, Verne and Zola.\n\n\nWhat can it do ?\n\n\nThe building of this tool is based on the article \nCharacter-level Convolutional Networks for Text\nClassification\n by X. Zhiang and al. Their model was used on various dataset in order to fulfill various tasks, such as Image Classification or Polarity Analysis. \n\n\nTiramisu's model has been optimized for the Author Classification. But what does it mean ?\n\n\nGiven a set of authors, we have trained a neural network to classify texts according to the set of authors. In other words, the network will take a text of length >= 376 characters as an input and will try to determine the author of the set whom most likely wrote it. Currently, its accuracy reaches 80%.\n\n\nWhat is in the dataset ?\n\n\nAs stated above, the network needs to be trained with a group of texts from the authors chosen.\n\n\nThe dataset has been created with the \ncrawler\n of the project Tiramisu. Basically, it is a structure composed of text files containing novels or short stories concatenated together. In this structure, each author has one input file that can be found under Text/Author/Result/inputfile.txt.\n\n\nThe program will run ONLY if the following architecture is followed:\n\n\nText>|----------Author1>|Result>|input_auth1.txt\n     |----------Author2>|Result>|input_auth2.txt\n     |----------Author3>|Result>|input_auth3.txt\nclassifier.py\n\n\n\nHow to use it ?\n\n\nIf you want to try out our solution on our dataset, no modification has to be done. Simply launch the following command:\n\n\npython3 classifier.py\n\n\n\n\nIf you want to try out our network on a given input and see who most likely wrote it, simply launch the following command:\n\n\npython3 classifier.py --analyze FILE_TO_ANALYSE\n\n\n\n\nIf you want to try out our network on your own data, you will have to load your data and train the network first. To do that, launch the following command:\n\n\npython3 classifier.py --train --newdata\n\n\n[CAUTION]\n This can take a LONG amount of time.\n\n\n\n\nNeed a more concrete / detailed example ? \nFollow me !",
            "title": "Author recognition"
        },
        {
            "location": "/author_recognition_guide/#getting-started-with-the-author-recognition-software",
            "text": "The dataset used for the following can be found  here . It contains texts from Balzac, France, Hugo, Maupassant, Proust, Verne and Zola.",
            "title": "Getting started with the author recognition software"
        },
        {
            "location": "/author_recognition_guide/#what-can-it-do",
            "text": "The building of this tool is based on the article  Character-level Convolutional Networks for Text\nClassification  by X. Zhiang and al. Their model was used on various dataset in order to fulfill various tasks, such as Image Classification or Polarity Analysis.   Tiramisu's model has been optimized for the Author Classification. But what does it mean ?  Given a set of authors, we have trained a neural network to classify texts according to the set of authors. In other words, the network will take a text of length >= 376 characters as an input and will try to determine the author of the set whom most likely wrote it. Currently, its accuracy reaches 80%.",
            "title": "What can it do ?"
        },
        {
            "location": "/author_recognition_guide/#what-is-in-the-dataset",
            "text": "As stated above, the network needs to be trained with a group of texts from the authors chosen.  The dataset has been created with the  crawler  of the project Tiramisu. Basically, it is a structure composed of text files containing novels or short stories concatenated together. In this structure, each author has one input file that can be found under Text/Author/Result/inputfile.txt.  The program will run ONLY if the following architecture is followed:  Text>|----------Author1>|Result>|input_auth1.txt\n     |----------Author2>|Result>|input_auth2.txt\n     |----------Author3>|Result>|input_auth3.txt\nclassifier.py",
            "title": "What is in the dataset ?"
        },
        {
            "location": "/author_recognition_guide/#how-to-use-it",
            "text": "If you want to try out our solution on our dataset, no modification has to be done. Simply launch the following command:  python3 classifier.py   If you want to try out our network on a given input and see who most likely wrote it, simply launch the following command:  python3 classifier.py --analyze FILE_TO_ANALYSE   If you want to try out our network on your own data, you will have to load your data and train the network first. To do that, launch the following command:  python3 classifier.py --train --newdata  [CAUTION]  This can take a LONG amount of time.   Need a more concrete / detailed example ?  Follow me !",
            "title": "How to use it ?"
        },
        {
            "location": "/text_generation_guide/",
            "text": "CURRENTLY UNDER CONSTRUCTION",
            "title": "Text generation"
        },
        {
            "location": "/text_generation_guide/#currently-under-construction",
            "text": "",
            "title": "CURRENTLY UNDER CONSTRUCTION"
        },
        {
            "location": "/crawler_guide/",
            "text": "Getting started with the Gutenberg crawler\n\n\nWARNING !\n\n\nThe robot crawling of their website is prohibited by gutenberg for anyone outside of the ESIEA. This crawler was done with the organization approval because the mirroring system of the Gutenberg website was down at the moment of the project. Before using it, please check if their mirroring system works : \nGutenberg mirrors\n\n\nWhat can it do ?\n\n\nThe present \ncrawler\n has been developped to download texts made available by the \nGutenberg Organization\n. Currently, you can either select to download texts by style or texts of a given language. \nNote: only french is supported at the moment.\n\n\nA \ncleaner\n has also been created; the texts downloaded from the Gutenberg Organization are watermarked. In order for our network to work, those watermarks need to be removed.\n\n\nFinally, in order to navigate smoothly between the downloaded files, a \nfinder\n has been implemented. Simply enter a sentence and it will tell you which file contains it.\n\n\nDownloading the texts\n\n\nThe crawler developed for the Tiramisu project can be used as follow:\n\n\npython crawl_gutenberg.py --nb_files NB_FILES --out_dir OUT_DIR --crawl_type CRAWL_TYPE\n\n\n\n\nNB_FILES\n: \ninteger\n indicating the number of files that will be downloaded by the crawler. Default is 100.\n\n\nOUT_DIR\n: \nstring\n indicating the path to the directory in which the files will be stored. Default is './'\n\n\nCRAWL_TYPE\n: \nstring\n indicating the type of crawling to be done. Currently, the supported crawling types are 'authors' and 'styles'. Default is 'authors'.\n\n\n--help\n, \n-h\n : print this help to the terminal.\n\n\n\n\n\n\nThe crawler currently only supports french language. More languages will be added soon.\n\n\nCleaning the texts\n\n\nIf you look into the downloaded texts, you will notice that they have a particular structure, such as:\n\n\n\n\nThis eBook is for the use of anyone anywhere at no cost and with\nalmost no restrictions whatsoever.  You may copy it, give it away or\nre-use it under the terms of the Project Gutenberg License included\nwith this eBook or online at www.gutenberg.net\n\n\nTitle: Histoire comique\n\n\nAuthor: Anatole France\n\n\nRelease Date: December 18, 2005 [EBook #17345]\n\n\nLanguage: French\n\n\nCharacter set encoding: UTF-8\n\n\n*** START OF THIS PROJECT GUTENBERG EBOOK HISTOIRE COMIQUE ***\n\n\n                  . . .\n\n\n\nBALTHASAR                                                1 vol.\n\n\nLE CRIME DE SYLVESTRE BONNARD (_Ouvrage couronn\u00e9\n\n\npar l'Acad\u00e9mie fran\u00e7aise_)                               1 --\n\n\nL'\u00c9TUI DE NACRE                                          1 --\n\n\nLE JARDIN D'\u00c9PICURE                                      1 --\n\n\n                  . . .\n\n\n\nI\n\n\nC'\u00e9tait dans une loge d'actrice, \u00e0 l'Od\u00e9on. Sous la lampe\n\u00e9lectrique, F\u00e9licie Nanteuil, la t\u00eate poudr\u00e9e, du bleu aux\npaupi\u00e8res, du rouge aux joues et aux oreilles, du blanc au cou et\n\n\n\n\nObviously, those information aren't relevant for the analysis of the text itself. It will corrupt our data; its removal is necessary.\n\n\n\n\nThe list of the removed pattern are the following:\n\n\n\n\nSTART OF\n ... \nEND OF\n\n\nProject Gutenberg\n ... \nsubscribe.\n\n\n< tag >\n ... \n< /tag >\n\n\n[\n...\n]\n\n\n(\n...\n)\n\n\n_\n\n\n\n\nThe list of the converted pattern are the following:\n\n\n\n\n--\n to \n\u2014\n\n\n<<\n to \n\u00ab\n\n\n>>\n to \n\u00bb\n \n\n\n\n\n\n\nThe cleaner developed for the Tiramisu project can be used as follow:\n\n\npython clean_text.py --nb_files NB_FILES --in_dir IN_DIR out_dir OUT_DIR --name NAME\n\n\n\n\nNB_FILES\n: \ninteger\n indicating the number of files cleaned by the cleaner. Default is 100.\n\n\nOUT_DIR\n: \nstring\n indicating the path in which the output file will be stored. Default is 'data/french/input.txt'\n\n\nNAME\n: \nstring\n indicating the name of the file to be created. Default is 'input.txt'\n\n\n\n\nNOTE\n\n\nPlease ensure that your text don't contain any unclosed parenthesis or bracket. A \nfunction\n has been developed to help you do that.\n\n\nFinding a text\n\n\nA finder has been implemented. If you need to look which files contains a specific sentence, just type:\n\n\npython findtext.py 'sentence'\n\n\nThe program will then output you a list of files in which the sentence has been found. If \nGedit\n is installed, the concerned file will be directly opened in it.\n\n\n\n\nNeed a more concrete / detailed example ? \nFollow me !",
            "title": "Gutenberg crawler"
        },
        {
            "location": "/crawler_guide/#getting-started-with-the-gutenberg-crawler",
            "text": "",
            "title": "Getting started with the Gutenberg crawler"
        },
        {
            "location": "/crawler_guide/#warning",
            "text": "The robot crawling of their website is prohibited by gutenberg for anyone outside of the ESIEA. This crawler was done with the organization approval because the mirroring system of the Gutenberg website was down at the moment of the project. Before using it, please check if their mirroring system works :  Gutenberg mirrors",
            "title": "WARNING !"
        },
        {
            "location": "/crawler_guide/#what-can-it-do",
            "text": "The present  crawler  has been developped to download texts made available by the  Gutenberg Organization . Currently, you can either select to download texts by style or texts of a given language.  Note: only french is supported at the moment.  A  cleaner  has also been created; the texts downloaded from the Gutenberg Organization are watermarked. In order for our network to work, those watermarks need to be removed.  Finally, in order to navigate smoothly between the downloaded files, a  finder  has been implemented. Simply enter a sentence and it will tell you which file contains it.",
            "title": "What can it do ?"
        },
        {
            "location": "/crawler_guide/#downloading-the-texts",
            "text": "The crawler developed for the Tiramisu project can be used as follow:  python crawl_gutenberg.py --nb_files NB_FILES --out_dir OUT_DIR --crawl_type CRAWL_TYPE   NB_FILES :  integer  indicating the number of files that will be downloaded by the crawler. Default is 100.  OUT_DIR :  string  indicating the path to the directory in which the files will be stored. Default is './'  CRAWL_TYPE :  string  indicating the type of crawling to be done. Currently, the supported crawling types are 'authors' and 'styles'. Default is 'authors'.  --help ,  -h  : print this help to the terminal.    The crawler currently only supports french language. More languages will be added soon.",
            "title": "Downloading the texts"
        },
        {
            "location": "/crawler_guide/#cleaning-the-texts",
            "text": "If you look into the downloaded texts, you will notice that they have a particular structure, such as:   This eBook is for the use of anyone anywhere at no cost and with\nalmost no restrictions whatsoever.  You may copy it, give it away or\nre-use it under the terms of the Project Gutenberg License included\nwith this eBook or online at www.gutenberg.net  Title: Histoire comique  Author: Anatole France  Release Date: December 18, 2005 [EBook #17345]  Language: French  Character set encoding: UTF-8  *** START OF THIS PROJECT GUTENBERG EBOOK HISTOIRE COMIQUE ***                    . . .  BALTHASAR                                                1 vol.  LE CRIME DE SYLVESTRE BONNARD (_Ouvrage couronn\u00e9  par l'Acad\u00e9mie fran\u00e7aise_)                               1 --  L'\u00c9TUI DE NACRE                                          1 --  LE JARDIN D'\u00c9PICURE                                      1 --                    . . .  I  C'\u00e9tait dans une loge d'actrice, \u00e0 l'Od\u00e9on. Sous la lampe\n\u00e9lectrique, F\u00e9licie Nanteuil, la t\u00eate poudr\u00e9e, du bleu aux\npaupi\u00e8res, du rouge aux joues et aux oreilles, du blanc au cou et   Obviously, those information aren't relevant for the analysis of the text itself. It will corrupt our data; its removal is necessary.   The list of the removed pattern are the following:   START OF  ...  END OF  Project Gutenberg  ...  subscribe.  < tag >  ...  < /tag >  [ ... ]  ( ... )  _   The list of the converted pattern are the following:   --  to  \u2014  <<  to  \u00ab  >>  to  \u00bb      The cleaner developed for the Tiramisu project can be used as follow:  python clean_text.py --nb_files NB_FILES --in_dir IN_DIR out_dir OUT_DIR --name NAME   NB_FILES :  integer  indicating the number of files cleaned by the cleaner. Default is 100.  OUT_DIR :  string  indicating the path in which the output file will be stored. Default is 'data/french/input.txt'  NAME :  string  indicating the name of the file to be created. Default is 'input.txt'   NOTE  Please ensure that your text don't contain any unclosed parenthesis or bracket. A  function  has been developed to help you do that.",
            "title": "Cleaning the texts"
        },
        {
            "location": "/crawler_guide/#finding-a-text",
            "text": "A finder has been implemented. If you need to look which files contains a specific sentence, just type:  python findtext.py 'sentence'  The program will then output you a list of files in which the sentence has been found. If  Gedit  is installed, the concerned file will be directly opened in it.   Need a more concrete / detailed example ?  Follow me !",
            "title": "Finding a text"
        },
        {
            "location": "/author_recognition/",
            "text": "Author Recognition\n\n\nOverview\n\n\nThe author recognition module is based on a convolutional network coupled with a Fully Connected network. It relies on the framework \nKeras\n for the neural network implementation and on the library \nSklearn\n for the analysis of the results.\n\n\nThe aim of this module is simple: \nbeing able to determine the author of a text accordingly to its writing style.\n\n\nIn order to do that, we need to have a large dataset. Relatively to our tests, we advise you to have at least 3MB of raw text per author.\n\n\nRequirements\n\n\nThe required libraries / frameworks for the project are the following:\n\n\n\n\nTensorflow >= 1.0\n\n\nNote: in order to run faster, please consider installing \nCUDA >= 8.5\n if your GPU supports it. The softwares included in the project can be up to 20x faster. In this case, please also install \nCuDNN5.1\n.\n\n\n\n\n\n\nKeras >= 2.0\n\n\nNumpy >= 1.12\n\n\nsudo pip3 install numpy\n will do the trick.\n\n\n\n\n\n\nSklearn >= 0.18.1\n\n\nsudo pip3 install scikit-learn\n will do the trick.\n\n\n\n\n\n\nMatplotlib >=  2.0.2\n\n\nsudo pip3 install matplotlib\n will do the trick. However, if some error occurs, please consider using \nsudo apt install python3-matplotlib\n\n\n\n\n\n\n\n\nModel details\n\n\nOur model is largely inspired by the paper \nCharacter-level Convolutional Networks for Text Classification\n. We adapted it to fit our purpose the best.\n\n\nArchitecture\n\n\nOur architecture is 8 layers deep, with 5 convolutional layers and 3 fully-connected layers.\n\n\nHere is a summary of the described architecture:\n\n\n\n\n\n\n\n\nLayer (type)\n\n\nOutput Shape\n\n\nParam #\n\n\n\n\n\n\n\n\n\n\nconv1d_1 (Conv1D)\n\n\n(None, 250, 256)\n\n\n46848\n\n\n\n\n\n\nmax_pooling1d_1 (MaxPooling1)\n\n\n(None, 125, 256)\n\n\n0\n\n\n\n\n\n\nconv1d_2 (Conv1D)\n\n\n(None, 123, 256)\n\n\n196864\n\n\n\n\n\n\nmax_pooling1d_2 (MaxPooling1)\n\n\n(None, 61, 256)\n\n\n0\n\n\n\n\n\n\nconv1d_3 (Conv1D)\n\n\n(None, 59, 256)\n\n\n196864\n\n\n\n\n\n\nconv1d_4 (Conv1D)\n\n\n(None, 57, 256)\n\n\n196864\n\n\n\n\n\n\nconv1d_5 (Conv1D)\n\n\n(None, 55, 256)\n\n\n196864\n\n\n\n\n\n\nmax_pooling1d_3 (MaxPooling1)\n\n\n(None, 27, 256)\n\n\n0\n\n\n\n\n\n\nflatten_1 (Flatten)\n\n\n(None, 6912)\n\n\n0\n\n\n\n\n\n\ndense_1 (Dense)\n\n\n(None, 1024)\n\n\n7078912\n\n\n\n\n\n\ndropout_1 (Dropout)\n\n\n(None, 1024)\n\n\n0\n\n\n\n\n\n\ndense_2 (Dense)\n\n\n(None, 1024)\n\n\n1049600\n\n\n\n\n\n\ndropout_2 (Dropout)\n\n\n(None, 1024)\n\n\n0\n\n\n\n\n\n\ndense_3 (Dense)\n\n\n(None, 5)\n\n\n5125\n\n\n\n\n\n\n\n\nDataset and Input\n\n\nThe solution we've implemented is character-level based. This means that we encode each character and then group some of them together in a matrix and then give it to the network. The encoding used is the \none-hot encoding\n. We've decided to only deal with the 26 letters of the alphabet, lowercased. Here is a convenient example:\n\n\nAlphabet : ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n\na -> [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nb -> [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n...\ny -> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\nz -> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n\n\n\nOnce our characters have been converted, we group them together. For instance, the sentence \nI am the\n will be encoded as:\n\n\n[[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], #i\n[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], #a\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], #m\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], #t\n[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], #h\n[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]] #e\n\n\n\nLet's say that our dataset contains texts from 3 authors: Maupassant, Hugo and Verne. Those authors can be encoded just like the letters. For instance:\n\n\n[1,0,0] # Maupassant\n[0,1,0] # Hugo\n[0,0,1] # Verne\n\n\n\nIn the end, samples composing our dataset have the following shape:\n\n\n(Matrix of encoded letters, Encoded author)\n\n([[0,0,...,1,0], \n  [0,0,...,0,0],\n       ...          # Matrix of letters     \n  [0,1,...,0,0],\n  [0,0,...,0,0]]\n  ,\n  [0,0,1])          # Author (also called target)\n\n\n\n\n\nThe dimension of a matrix is (\nalphabetSize\n, \nmaxFeature\n). MaxFeature is the number of consecutive letters we give to the network. You can imagine that as a fixed visual field: the network will determine an author based on this number of consecutive characters only.\n\n\nIn the paper we've used as a basis, maxFeature was set to 1014. The value we've used has been obtained the following way:\n\n\nTiramisuMaxFeature = (PaperMaxFeature/PaperAlphabetLen) * TiramisuAlphabetLen\n(1014/77) * 26 = 376\n\n\n\nAccording to our tests, this value seems the most adapted to our problem.\n\n\nWe have estimated that at least 3MB of raw txt files per author is required to ensure good results.\n\n\nOutput\n\n\nThe last layer of the network will output a vector of size equal to the number of authors in the dataset. This vector will contain values ranged from 0 to 1, indicating the likelihood relative to each author. Here is an example of the output:\n\n\n           Auth1   Auth2  Auth3  Auth4  Auth5\nResults: [ 0.670 , 0.030, 0.050, 0.120, 0.130]\n\n\n\nData processing\n\n\nload_data()\n\n\nparams:\n None\n\n\nreturn:\n \nlist\n, a list containing respectively the training matrices, the training targets, the validation matrices and the validation targets.\n\n\n\n\nBehaviour\n\n\nThis function will load the data and convert it to the format discussed \nabove\n.\n\n\nIn order to easily perform some tasks, the data will be stored as numpy arrays instead of simple lists.\n\n\n\n\nModel training\n\n\ntrain_model(x_train, y_train, x_test, y_test)\n\n\nparams:\n\n\n\n\nx_train\n, \nlist of numpy arrays\n, list of size (alphabet_length, maxfeature). This will be the matrices of the training set.\n\n\ny_train\n, \nlist of numpy arrays\n, list representing the authors of the x_train matrices. This will be the targets of the training set.\n\n\nx_test\n, \nlist of numpy arrays\n, list of size (alphabet_length, maxfeature). This will be the matrices of the test set.\n\n\ny_test\n, \nlist of numpy arrays\n, list representing the authors of the x_test matrices. This will be the targets of the test set.\n\n\n\n\nreturn:\n \nKeras model object\n, the compiled and trained model we've created\n\n\n\n\nBehaviour\n\n\nThis function will build and train the network. A bunch of functionnality have been implemented, such as degressive learning rates, custom activation or early / manual stopping of the training.\n\n\nAll the configuration of the network can be done within this function. The values found in the github are considered as our standard.\n\n\nFor more information about the compilation, the effective training or the tuning of the model, please refer to the \nKeras documentation\n.\n\n\n\n\nAnalysis\n\n\nprint_confusion_matrix(args, model, X_test, Y_test, save_dir)\n\n\nparams:\n\n\n\n\nargs\n, \nlist of strings\n, a parsed version of the entries given in the command line. For more information, follow \nthis link\n.\n\n\nmodel\n, \nKeras model object\n, the model we want to analyze\n\n\nX_test\n, \nlist of numpy arrays\n, list of size (alphabet_length, maxfeature). This will be the matrices of the test set.\n\n\nY_test\n, \nlist of numpy arrays\n, list representing the authors of the x_test matrices. This will be the targets of the test set.\n\n\nsave_dir\n, \nstring\n, path to the directory in which a report will be created\n\n\n\n\nreturn:\n \nKeras model object\n, the compiled and trained model we've created\n\n\n\n\nBehaviour\n\n\nThis function will generate a classification report, a confusion matrix (that can be saved as a png file) as well as a report file, listing the architecture and the hyperparameters of the model.\n\n\nHere is an example of a generated confusion matrix:\n\n\n\n\nFor more information about the classification report or confusion matrix creation, please refer to the \nSklearn documentation\n.\n\n\n\n\nMiscellaneous\n\n\nget_auth_number()\n\n\nparams:\n None\n\n\nreturn:\n \nint\n, number of distinct authors found in the dataset\n\n\n\n\nBehaviour\n\n\nSimply iterate over the subfolders of \nText/\n and count the number of folder found.  \n\n\n\n\ndisplay_parameters()\n\n\nparams:\n None\n\n\nreturn:\n None\n\n\n\n\nBehaviour\n\n\nDisplay all the parameters of the model in the terminal.\n\n\nDeprecated function\n. Might be enabled again when reworked.",
            "title": "Author recognition"
        },
        {
            "location": "/author_recognition/#author-recognition",
            "text": "",
            "title": "Author Recognition"
        },
        {
            "location": "/author_recognition/#overview",
            "text": "The author recognition module is based on a convolutional network coupled with a Fully Connected network. It relies on the framework  Keras  for the neural network implementation and on the library  Sklearn  for the analysis of the results.  The aim of this module is simple:  being able to determine the author of a text accordingly to its writing style.  In order to do that, we need to have a large dataset. Relatively to our tests, we advise you to have at least 3MB of raw text per author.",
            "title": "Overview"
        },
        {
            "location": "/author_recognition/#requirements",
            "text": "The required libraries / frameworks for the project are the following:   Tensorflow >= 1.0  Note: in order to run faster, please consider installing  CUDA >= 8.5  if your GPU supports it. The softwares included in the project can be up to 20x faster. In this case, please also install  CuDNN5.1 .    Keras >= 2.0  Numpy >= 1.12  sudo pip3 install numpy  will do the trick.    Sklearn >= 0.18.1  sudo pip3 install scikit-learn  will do the trick.    Matplotlib >=  2.0.2  sudo pip3 install matplotlib  will do the trick. However, if some error occurs, please consider using  sudo apt install python3-matplotlib",
            "title": "Requirements"
        },
        {
            "location": "/author_recognition/#model-details",
            "text": "Our model is largely inspired by the paper  Character-level Convolutional Networks for Text Classification . We adapted it to fit our purpose the best.",
            "title": "Model details"
        },
        {
            "location": "/author_recognition/#architecture",
            "text": "Our architecture is 8 layers deep, with 5 convolutional layers and 3 fully-connected layers.  Here is a summary of the described architecture:     Layer (type)  Output Shape  Param #      conv1d_1 (Conv1D)  (None, 250, 256)  46848    max_pooling1d_1 (MaxPooling1)  (None, 125, 256)  0    conv1d_2 (Conv1D)  (None, 123, 256)  196864    max_pooling1d_2 (MaxPooling1)  (None, 61, 256)  0    conv1d_3 (Conv1D)  (None, 59, 256)  196864    conv1d_4 (Conv1D)  (None, 57, 256)  196864    conv1d_5 (Conv1D)  (None, 55, 256)  196864    max_pooling1d_3 (MaxPooling1)  (None, 27, 256)  0    flatten_1 (Flatten)  (None, 6912)  0    dense_1 (Dense)  (None, 1024)  7078912    dropout_1 (Dropout)  (None, 1024)  0    dense_2 (Dense)  (None, 1024)  1049600    dropout_2 (Dropout)  (None, 1024)  0    dense_3 (Dense)  (None, 5)  5125",
            "title": "Architecture"
        },
        {
            "location": "/author_recognition/#dataset-and-input",
            "text": "The solution we've implemented is character-level based. This means that we encode each character and then group some of them together in a matrix and then give it to the network. The encoding used is the  one-hot encoding . We've decided to only deal with the 26 letters of the alphabet, lowercased. Here is a convenient example:  Alphabet : ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n\na -> [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nb -> [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n...\ny -> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\nz -> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  Once our characters have been converted, we group them together. For instance, the sentence  I am the  will be encoded as:  [[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], #i\n[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], #a\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], #m\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], #t\n[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], #h\n[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]] #e  Let's say that our dataset contains texts from 3 authors: Maupassant, Hugo and Verne. Those authors can be encoded just like the letters. For instance:  [1,0,0] # Maupassant\n[0,1,0] # Hugo\n[0,0,1] # Verne  In the end, samples composing our dataset have the following shape:  (Matrix of encoded letters, Encoded author)\n\n([[0,0,...,1,0], \n  [0,0,...,0,0],\n       ...          # Matrix of letters     \n  [0,1,...,0,0],\n  [0,0,...,0,0]]\n  ,\n  [0,0,1])          # Author (also called target)   The dimension of a matrix is ( alphabetSize ,  maxFeature ). MaxFeature is the number of consecutive letters we give to the network. You can imagine that as a fixed visual field: the network will determine an author based on this number of consecutive characters only.  In the paper we've used as a basis, maxFeature was set to 1014. The value we've used has been obtained the following way:  TiramisuMaxFeature = (PaperMaxFeature/PaperAlphabetLen) * TiramisuAlphabetLen\n(1014/77) * 26 = 376  According to our tests, this value seems the most adapted to our problem.  We have estimated that at least 3MB of raw txt files per author is required to ensure good results.",
            "title": "Dataset and Input"
        },
        {
            "location": "/author_recognition/#output",
            "text": "The last layer of the network will output a vector of size equal to the number of authors in the dataset. This vector will contain values ranged from 0 to 1, indicating the likelihood relative to each author. Here is an example of the output:             Auth1   Auth2  Auth3  Auth4  Auth5\nResults: [ 0.670 , 0.030, 0.050, 0.120, 0.130]",
            "title": "Output"
        },
        {
            "location": "/author_recognition/#data-processing",
            "text": "",
            "title": "Data processing"
        },
        {
            "location": "/author_recognition/#load_data",
            "text": "params:  None  return:   list , a list containing respectively the training matrices, the training targets, the validation matrices and the validation targets.",
            "title": "load_data()"
        },
        {
            "location": "/author_recognition/#behaviour",
            "text": "This function will load the data and convert it to the format discussed  above .  In order to easily perform some tasks, the data will be stored as numpy arrays instead of simple lists.",
            "title": "Behaviour"
        },
        {
            "location": "/author_recognition/#model-training",
            "text": "",
            "title": "Model training"
        },
        {
            "location": "/author_recognition/#train_modelx_train-y_train-x_test-y_test",
            "text": "params:   x_train ,  list of numpy arrays , list of size (alphabet_length, maxfeature). This will be the matrices of the training set.  y_train ,  list of numpy arrays , list representing the authors of the x_train matrices. This will be the targets of the training set.  x_test ,  list of numpy arrays , list of size (alphabet_length, maxfeature). This will be the matrices of the test set.  y_test ,  list of numpy arrays , list representing the authors of the x_test matrices. This will be the targets of the test set.   return:   Keras model object , the compiled and trained model we've created",
            "title": "train_model(x_train, y_train, x_test, y_test)"
        },
        {
            "location": "/author_recognition/#behaviour_1",
            "text": "This function will build and train the network. A bunch of functionnality have been implemented, such as degressive learning rates, custom activation or early / manual stopping of the training.  All the configuration of the network can be done within this function. The values found in the github are considered as our standard.  For more information about the compilation, the effective training or the tuning of the model, please refer to the  Keras documentation .",
            "title": "Behaviour"
        },
        {
            "location": "/author_recognition/#analysis",
            "text": "",
            "title": "Analysis"
        },
        {
            "location": "/author_recognition/#print_confusion_matrixargs-model-x_test-y_test-save_dir",
            "text": "params:   args ,  list of strings , a parsed version of the entries given in the command line. For more information, follow  this link .  model ,  Keras model object , the model we want to analyze  X_test ,  list of numpy arrays , list of size (alphabet_length, maxfeature). This will be the matrices of the test set.  Y_test ,  list of numpy arrays , list representing the authors of the x_test matrices. This will be the targets of the test set.  save_dir ,  string , path to the directory in which a report will be created   return:   Keras model object , the compiled and trained model we've created",
            "title": "print_confusion_matrix(args, model, X_test, Y_test, save_dir)"
        },
        {
            "location": "/author_recognition/#behaviour_2",
            "text": "This function will generate a classification report, a confusion matrix (that can be saved as a png file) as well as a report file, listing the architecture and the hyperparameters of the model.  Here is an example of a generated confusion matrix:   For more information about the classification report or confusion matrix creation, please refer to the  Sklearn documentation .",
            "title": "Behaviour"
        },
        {
            "location": "/author_recognition/#miscellaneous",
            "text": "",
            "title": "Miscellaneous"
        },
        {
            "location": "/author_recognition/#get_auth_number",
            "text": "params:  None  return:   int , number of distinct authors found in the dataset",
            "title": "get_auth_number()"
        },
        {
            "location": "/author_recognition/#behaviour_3",
            "text": "Simply iterate over the subfolders of  Text/  and count the number of folder found.",
            "title": "Behaviour"
        },
        {
            "location": "/author_recognition/#display_parameters",
            "text": "params:  None  return:  None",
            "title": "display_parameters()"
        },
        {
            "location": "/author_recognition/#behaviour_4",
            "text": "Display all the parameters of the model in the terminal.  Deprecated function . Might be enabled again when reworked.",
            "title": "Behaviour"
        },
        {
            "location": "/text_generation/",
            "text": "CURRENTLY UNDER CONSTRUCTION",
            "title": "Text generation"
        },
        {
            "location": "/text_generation/#currently-under-construction",
            "text": "",
            "title": "CURRENTLY UNDER CONSTRUCTION"
        },
        {
            "location": "/crawler/",
            "text": "Crawler\n\n\nOverview\n\n\nThe crawler is split between three files, \ncrawl_gutenberg.py\n, \nclean_text.py\n, \nfindtext.py\n:\n\n\n\n\n\n\ncrawl_gutenberg.py\n contains the functions \ncrawler()\n and \ncompressFiles()\n. \n\n\n\n\n\n\nclean_text.py\n contains the functions \nconcatenateFiles()\n and \ncleanText()\n.\n\n\n\n\n\n\nfindtext.py\n contains the functions \ngetNameText()\n and \nfindAllIncorrectTexts()\n.\n\n\n\n\n\n\nRequirements\n\n\nThe required libraries / frameworks for the project are the following:\n\n\n\n\nhttplib2\n\n\npip3 install httplib2\n should do the trick\n\n\n\n\n\n\nrequests\n\n\npip3 install requests\n should do the trick\n\n\n\n\n\n\nBeautiful Soup >= 4.0\n\n\npip3 install bs4\n should do the trick\n\n\n\n\n\n\n\n\nCrawl_gutenberg.py\n\n\ncrawler(args)\n\n\nparams:\n \nargs\n, \nlist of strings\n, a parsed version of the entries given in the command line. For more information, follow \nthis link\n.\n\n\nreturn:\n None\n\n\n\n\nBehaviour\n\n\nThis function will effectively crawl the website gutenberg.org to find the books and then download them. It will first access the \nFrench page\n, crawl it and extract all the \n<href>\n tags on it. If the \ncrawl_type\n argument is \n'styles'\n, it will look for specific styles, such as \nTh\u00e9\u00e2tre\n or \nNouvelles\n. Otherwise, it will look for any french book.\n\n\nOnce the books identified, the crawler will access each one of them one by one, download it and store it in the specified folder \n(default: './'. Note: a subdir gutenberg will be created.)\n.\n\n\nThe function will stop once the number of files downloaded has reached the argument \nnb_file\n.\n\n\n\n\ncompressFiles(args)\n\n\nparams:\n args, a parsed version of the entries given in the command line. For more information, follow \nthis link\n.\n\n\nreturn:\n None\n\n\n\n\nBehaviour\n\n\nThis function will zip the files together using \nzipfile\n. If there is no file in the out directory, the program will exit. Otherwise, it will add all the files in the directory to a zip file named <out_dir>.zip, located in the current directory.  \n\n\nClean_text.py\n\n\nLOGGER INCLUDED !\n\n\nconcatenateFiles(inputDirectory, outputFile, args)\n\n\nparams:\n\n\n\n\n\n\ninputDirectory, the directory where the files to           clean are stored\n\n\n\n\n\n\noutputFile, the path to the file to be created\n\n\n\n\n\n\nargs, a parsed version of the entries given in the command line. For more information, follow \nthis link\n.\n\n\n\n\n\n\nreturn:\n None\n\n\n\n\nBehaviour\n\n\nThis function will concatenate the files contained in the directory \ninputDirectory\n and store the result in a single file named \noutputFile\n (\nNote: outputFile is in fact a path\n).\n\n\nIf no files are in the directory \ninputDirectory\n, the function will look for a zip file in the current directory. In a zipfile is present, it will extract the files and then concatenate them. Otherwise, the program will terminate.\n\n\n\n\ncleanText(inputDirectory, outputFile, args)\n\n\nparams:\n \n\n\n\n\n\n\ninputDirectory, the directory where the files to           clean are stored\n\n\n\n\n\n\noutputFile, the path to the file to be created\n\n\n\n\n\n\nargs, a parsed version of the entries given in the command line. For more information, follow \nthis link\n.\n\n\n\n\n\n\nreturn:\n None\n\n\n\n\nBehaviour\n\n\nThis function will remove many undesired characters or pattern from a given text file by scanning it character by character. The list of the removed pattern can be found \nhere\n.\n\n\n\n\nFindtext.py\n\n\ngetNameText(string, cmd)\n\n\nparams:\n \n\n\n\n\n\n\nstring, a string that we want to find in one of the text we have, downloaded by the crawler. Default is ''.\n\n\n\n\n\n\neditor, a string containing the command to launch a text editor, e.g. \"gedit\" for Gedit or \"subl\" for Sublime Text. Default is 'gedit'.\n\n\n\n\n\n\nreturn:\n None\n\n\n\n\nBehaviour\n\n\nThis function will look for the input string in the text files that we have in the directory of the downloaded files. It will then print the name of the files containing it, and open them in the editor specified. (\nNote: for the moment, the directory can only be 'gutenberg'. Adjustements should be done soon\n)\n\n\n\n\nfindAllIncorrectTexts(inputDirectory)\n\n\nparams:\n inputDirectory, the directory where the files to analyze are stored.\n\n\nreturn:\n a list containing the incorrect files names.\n\n\n\n\nBehaviour\n\n\nThis function will check if the files of the specified directory don't have any unclosed parenthesis or bracket. Those files MUST be excluded from the cleaning process.",
            "title": "Gutenberg crawler"
        },
        {
            "location": "/crawler/#crawler",
            "text": "",
            "title": "Crawler"
        },
        {
            "location": "/crawler/#overview",
            "text": "The crawler is split between three files,  crawl_gutenberg.py ,  clean_text.py ,  findtext.py :    crawl_gutenberg.py  contains the functions  crawler()  and  compressFiles() .     clean_text.py  contains the functions  concatenateFiles()  and  cleanText() .    findtext.py  contains the functions  getNameText()  and  findAllIncorrectTexts() .",
            "title": "Overview"
        },
        {
            "location": "/crawler/#requirements",
            "text": "The required libraries / frameworks for the project are the following:   httplib2  pip3 install httplib2  should do the trick    requests  pip3 install requests  should do the trick    Beautiful Soup >= 4.0  pip3 install bs4  should do the trick",
            "title": "Requirements"
        },
        {
            "location": "/crawler/#crawl_gutenbergpy",
            "text": "",
            "title": "Crawl_gutenberg.py"
        },
        {
            "location": "/crawler/#crawlerargs",
            "text": "params:   args ,  list of strings , a parsed version of the entries given in the command line. For more information, follow  this link .  return:  None",
            "title": "crawler(args)"
        },
        {
            "location": "/crawler/#behaviour",
            "text": "This function will effectively crawl the website gutenberg.org to find the books and then download them. It will first access the  French page , crawl it and extract all the  <href>  tags on it. If the  crawl_type  argument is  'styles' , it will look for specific styles, such as  Th\u00e9\u00e2tre  or  Nouvelles . Otherwise, it will look for any french book.  Once the books identified, the crawler will access each one of them one by one, download it and store it in the specified folder  (default: './'. Note: a subdir gutenberg will be created.) .  The function will stop once the number of files downloaded has reached the argument  nb_file .",
            "title": "Behaviour"
        },
        {
            "location": "/crawler/#compressfilesargs",
            "text": "params:  args, a parsed version of the entries given in the command line. For more information, follow  this link .  return:  None",
            "title": "compressFiles(args)"
        },
        {
            "location": "/crawler/#behaviour_1",
            "text": "This function will zip the files together using  zipfile . If there is no file in the out directory, the program will exit. Otherwise, it will add all the files in the directory to a zip file named <out_dir>.zip, located in the current directory.",
            "title": "Behaviour"
        },
        {
            "location": "/crawler/#clean_textpy",
            "text": "LOGGER INCLUDED !",
            "title": "Clean_text.py"
        },
        {
            "location": "/crawler/#concatenatefilesinputdirectory-outputfile-args",
            "text": "params:    inputDirectory, the directory where the files to           clean are stored    outputFile, the path to the file to be created    args, a parsed version of the entries given in the command line. For more information, follow  this link .    return:  None",
            "title": "concatenateFiles(inputDirectory, outputFile, args)"
        },
        {
            "location": "/crawler/#behaviour_2",
            "text": "This function will concatenate the files contained in the directory  inputDirectory  and store the result in a single file named  outputFile  ( Note: outputFile is in fact a path ).  If no files are in the directory  inputDirectory , the function will look for a zip file in the current directory. In a zipfile is present, it will extract the files and then concatenate them. Otherwise, the program will terminate.",
            "title": "Behaviour"
        },
        {
            "location": "/crawler/#cleantextinputdirectory-outputfile-args",
            "text": "params:      inputDirectory, the directory where the files to           clean are stored    outputFile, the path to the file to be created    args, a parsed version of the entries given in the command line. For more information, follow  this link .    return:  None",
            "title": "cleanText(inputDirectory, outputFile, args)"
        },
        {
            "location": "/crawler/#behaviour_3",
            "text": "This function will remove many undesired characters or pattern from a given text file by scanning it character by character. The list of the removed pattern can be found  here .",
            "title": "Behaviour"
        },
        {
            "location": "/crawler/#findtextpy",
            "text": "",
            "title": "Findtext.py"
        },
        {
            "location": "/crawler/#getnametextstring-cmd",
            "text": "params:      string, a string that we want to find in one of the text we have, downloaded by the crawler. Default is ''.    editor, a string containing the command to launch a text editor, e.g. \"gedit\" for Gedit or \"subl\" for Sublime Text. Default is 'gedit'.    return:  None",
            "title": "getNameText(string, cmd)"
        },
        {
            "location": "/crawler/#behaviour_4",
            "text": "This function will look for the input string in the text files that we have in the directory of the downloaded files. It will then print the name of the files containing it, and open them in the editor specified. ( Note: for the moment, the directory can only be 'gutenberg'. Adjustements should be done soon )",
            "title": "Behaviour"
        },
        {
            "location": "/crawler/#findallincorrecttextsinputdirectory",
            "text": "params:  inputDirectory, the directory where the files to analyze are stored.  return:  a list containing the incorrect files names.",
            "title": "findAllIncorrectTexts(inputDirectory)"
        },
        {
            "location": "/crawler/#behaviour_5",
            "text": "This function will check if the files of the specified directory don't have any unclosed parenthesis or bracket. Those files MUST be excluded from the cleaning process.",
            "title": "Behaviour"
        },
        {
            "location": "/example_auth/",
            "text": "Author Recognition example\n\n\nOur network can be used in various ways. If you want to train it on your own data, \nfollow me here\n.\n\n\nIf you prefer to test our solution on our data to assess its reliability, \nfollow me here\n.\n\n\nIf you just want to know who wrote a piece of text, \nfollow me here\n.\n\n\nFinally, if your plan is to convert our network to a Pacman solver, feel free to do it. But please send us a video!\n\n\n\n\nTraining on your own data\n\n\nIf you want the network to consider specific authors, you will need to train the network from scratch. This can be a long process, but there is no workaround. In order to gain time, please install \nCuda\n. Otherwise ... Well, launch the program before leaving on vacations because it will take too much time. Really. \nInstall it\n. \n\n\nBuild your database\n\n\nCuda has been correctly set up ? Congratulations ! You've done the hardest part. Now, let's build your database.\n\n\nSet up the architecture\n\n\nPick the books you want to work on and create a directory named \nText\n. After that, create one subdirectory per author you want to have in your database, and name it after him/her.\n\n\nThe architecture of your Text directory should be the following:\n\n\n\n\nOnce created, move the correct files to the correct directories.\n\n\nIn every subdirectory you have created, create a new subdirectory called Result. That is the place we will store the clean file our network will take as an input.\n\n\nEach one of your directory should look like this now:\n\n\n\n\n\n\nCleaning the texts\n\n\nNow that you have all your files under your hand, it's time to clean and concatenate them.\n\n\nIn order to do that, issue the following command from Tiramisu's root directory:\n\n\npython3 clean_text.py --in_dir 'Text' --out_dir 'Result'\n\n\nThe program will run for a while and then exit. If the program encounters an error, please launch the \nfindIncorrectTexts()\n script and remove the incorrect files.\n\n\n\n\nTraining the network\n\n\nBuilding the network is simple. Just issue the following command:\n\n\npython3 classifier.py --newdata --train\n\n\nThe above command will result in three things: \nbuilding\n of the database, \nbuilding\n of the model and \ntraining\n of the model.\n\n\nDepending on the available computing power you have, the two first might take minutes to complete without any animation. After this while, you should end up seeing something like this:\n\n\npython3 PSTv2.py --train -m 'Tried 2048 hidden units instead of 1024'Using TensorFlow backend.\nLoading done : 32890\nLogging directory : ./logs/CNN_MaxF376_BS128_LenAlph26/33\n\n[Building Model]\nTrain on 32890 samples, validate on 14097 samples\n2017-05-18 16:49:08.816226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \nname: Quadro K620\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.124\npciBusID 0000:01:00.0\nTotal memory: 1.95GiB\nFree memory: 1.73GiB\n2017-05-18 16:49:08.816237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 \n2017-05-18 16:49:08.816241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y \n2017-05-18 16:49:08.816249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro K620, pci bus id: 0000:01:00.0)\nEpoch 1/300:\n4480/32890 [===>..........................] - ETA: 133s - loss: 2.1511 - categorical_accuracy: 0.2248\n\n\n\n\nGood ! Now all you have to do is wait. If you want to stop the network from training, wait for the epochs to end or hit \nCTRL+C\n. This won't exit the program, it will just end the training after the current epoch.\n\n\nNext step \nhere\n.\n\n\n\n\nTesting our implementation\n\n\nYou want to see how robust our model is? That's the spirit!\n\n\nAcquiring the dataset\n\n\nDownload\n our dataset as a numpy dumped array and place the zipfile in Tiramisu's root folder.\n\n\nRight-click on the obtained folder and unzip it.\n\n\nRunning the network\n\n\nSimply run the following command. It's as easy as that.\n\n\npython3 classifier.py\n\n\nNo error? Great! Next step \nhere\n.\n\n\n\n\nAnalyze a text\n\n\n \nUNDER CONSTRUCTION\n \n\n\n\n\nAnalyzing the results\n\n\nOn the go\n\n\nTensorflow has created a powerful tool called \nTensorboard\n. This tool enables us to have a live view of the behaviour of the network. In order to have access to Tensorboard, issue the following command from Tiramisu's root directory:\n\n\ntensorboard --logdir logs/\n\n\nThen, open your favorite web browser and navigate to \nlocalhost:6006\n. Click on the runs that interest you, and you should be able to see something like this:\n\n\n\n\n\n\nAt the end\n\n\nThe network is done training. Now, some reports should have been computed and here is what you should see:\n\n\nSuccessfully loaded model from ./save/last.hdf5\n\n\n[Reports creation]\n14080/14097 [============================>.] - ETA: 0s  \n\n     [Predictions]\n[0 1 1 ..., 1 0 0]\n\n\n     [Expected]\n[[ 1.  0.  0.  0.  0.]\n [ 1.  0.  0.  0.  0.]\n [ 0.  0.  0.  1.  0.]\n ..., \n [ 0.  0.  0.  1.  0.]\n [ 1.  0.  0.  0.  0.]\n [ 1.  0.  0.  0.  0.]]\n\n\n     [Reports]\n             [Classification Report]\n\n             precision    recall  f1-score   support\n\n     Balzac       0.85      0.78      0.81      3768\n       Hugo       0.67      0.77      0.72      3066\n       Zola       0.73      0.84      0.78      2415\n      Verne       0.88      0.76      0.81      2789\n     Proust       0.77      0.75      0.76      2059\n\navg / total       0.79      0.78      0.78     14097\n\n\n\n\n\nA clearer representation of the confusion matrix should also appear:\n\n\n\n\nWhat does it mean ? Basically, the darker on the diagonal the better. This matrix shows the predictions of the network relatively to the true labels.\n\n\n\n\nAn unexpected error occured ? Please refer to the \nFAQ\n, and if it doesn't help, open an \nissue\n on the github.",
            "title": "Author recognition"
        },
        {
            "location": "/example_auth/#author-recognition-example",
            "text": "Our network can be used in various ways. If you want to train it on your own data,  follow me here .  If you prefer to test our solution on our data to assess its reliability,  follow me here .  If you just want to know who wrote a piece of text,  follow me here .  Finally, if your plan is to convert our network to a Pacman solver, feel free to do it. But please send us a video!",
            "title": "Author Recognition example"
        },
        {
            "location": "/example_auth/#training-on-your-own-data",
            "text": "If you want the network to consider specific authors, you will need to train the network from scratch. This can be a long process, but there is no workaround. In order to gain time, please install  Cuda . Otherwise ... Well, launch the program before leaving on vacations because it will take too much time. Really.  Install it .",
            "title": "Training on your own data"
        },
        {
            "location": "/example_auth/#build-your-database",
            "text": "Cuda has been correctly set up ? Congratulations ! You've done the hardest part. Now, let's build your database.",
            "title": "Build your database"
        },
        {
            "location": "/example_auth/#set-up-the-architecture",
            "text": "Pick the books you want to work on and create a directory named  Text . After that, create one subdirectory per author you want to have in your database, and name it after him/her.  The architecture of your Text directory should be the following:   Once created, move the correct files to the correct directories.  In every subdirectory you have created, create a new subdirectory called Result. That is the place we will store the clean file our network will take as an input.  Each one of your directory should look like this now:",
            "title": "Set up the architecture"
        },
        {
            "location": "/example_auth/#cleaning-the-texts",
            "text": "Now that you have all your files under your hand, it's time to clean and concatenate them.  In order to do that, issue the following command from Tiramisu's root directory:  python3 clean_text.py --in_dir 'Text' --out_dir 'Result'  The program will run for a while and then exit. If the program encounters an error, please launch the  findIncorrectTexts()  script and remove the incorrect files.",
            "title": "Cleaning the texts"
        },
        {
            "location": "/example_auth/#training-the-network",
            "text": "Building the network is simple. Just issue the following command:  python3 classifier.py --newdata --train  The above command will result in three things:  building  of the database,  building  of the model and  training  of the model.  Depending on the available computing power you have, the two first might take minutes to complete without any animation. After this while, you should end up seeing something like this:  python3 PSTv2.py --train -m 'Tried 2048 hidden units instead of 1024'Using TensorFlow backend.\nLoading done : 32890\nLogging directory : ./logs/CNN_MaxF376_BS128_LenAlph26/33\n\n[Building Model]\nTrain on 32890 samples, validate on 14097 samples\n2017-05-18 16:49:08.816226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \nname: Quadro K620\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.124\npciBusID 0000:01:00.0\nTotal memory: 1.95GiB\nFree memory: 1.73GiB\n2017-05-18 16:49:08.816237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 \n2017-05-18 16:49:08.816241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y \n2017-05-18 16:49:08.816249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro K620, pci bus id: 0000:01:00.0)\nEpoch 1/300:\n4480/32890 [===>..........................] - ETA: 133s - loss: 2.1511 - categorical_accuracy: 0.2248  Good ! Now all you have to do is wait. If you want to stop the network from training, wait for the epochs to end or hit  CTRL+C . This won't exit the program, it will just end the training after the current epoch.  Next step  here .",
            "title": "Training the network"
        },
        {
            "location": "/example_auth/#testing-our-implementation",
            "text": "You want to see how robust our model is? That's the spirit!",
            "title": "Testing our implementation"
        },
        {
            "location": "/example_auth/#acquiring-the-dataset",
            "text": "Download  our dataset as a numpy dumped array and place the zipfile in Tiramisu's root folder.  Right-click on the obtained folder and unzip it.",
            "title": "Acquiring the dataset"
        },
        {
            "location": "/example_auth/#running-the-network",
            "text": "Simply run the following command. It's as easy as that.  python3 classifier.py  No error? Great! Next step  here .",
            "title": "Running the network"
        },
        {
            "location": "/example_auth/#analyze-a-text",
            "text": "UNDER CONSTRUCTION",
            "title": "Analyze a text"
        },
        {
            "location": "/example_auth/#analyzing-the-results",
            "text": "",
            "title": "Analyzing the results"
        },
        {
            "location": "/example_auth/#on-the-go",
            "text": "Tensorflow has created a powerful tool called  Tensorboard . This tool enables us to have a live view of the behaviour of the network. In order to have access to Tensorboard, issue the following command from Tiramisu's root directory:  tensorboard --logdir logs/  Then, open your favorite web browser and navigate to  localhost:6006 . Click on the runs that interest you, and you should be able to see something like this:",
            "title": "On the go"
        },
        {
            "location": "/example_auth/#at-the-end",
            "text": "The network is done training. Now, some reports should have been computed and here is what you should see:  Successfully loaded model from ./save/last.hdf5\n\n\n[Reports creation]\n14080/14097 [============================>.] - ETA: 0s  \n\n     [Predictions]\n[0 1 1 ..., 1 0 0]\n\n\n     [Expected]\n[[ 1.  0.  0.  0.  0.]\n [ 1.  0.  0.  0.  0.]\n [ 0.  0.  0.  1.  0.]\n ..., \n [ 0.  0.  0.  1.  0.]\n [ 1.  0.  0.  0.  0.]\n [ 1.  0.  0.  0.  0.]]\n\n\n     [Reports]\n             [Classification Report]\n\n             precision    recall  f1-score   support\n\n     Balzac       0.85      0.78      0.81      3768\n       Hugo       0.67      0.77      0.72      3066\n       Zola       0.73      0.84      0.78      2415\n      Verne       0.88      0.76      0.81      2789\n     Proust       0.77      0.75      0.76      2059\n\navg / total       0.79      0.78      0.78     14097  A clearer representation of the confusion matrix should also appear:   What does it mean ? Basically, the darker on the diagonal the better. This matrix shows the predictions of the network relatively to the true labels.   An unexpected error occured ? Please refer to the  FAQ , and if it doesn't help, open an  issue  on the github.",
            "title": "At the end"
        },
        {
            "location": "/example_textgen/",
            "text": "CURRENTLY UNDER CONSTRUCTION",
            "title": "Text generation"
        },
        {
            "location": "/example_textgen/#currently-under-construction",
            "text": "",
            "title": "CURRENTLY UNDER CONSTRUCTION"
        },
        {
            "location": "/example_crawler/",
            "text": "CURRENTLY UNDER CONSTRUCTION",
            "title": "Gutenberg Crawler"
        },
        {
            "location": "/example_crawler/#currently-under-construction",
            "text": "",
            "title": "CURRENTLY UNDER CONSTRUCTION"
        },
        {
            "location": "/resources/",
            "text": "Articles\n\n\nThe following articles were used as references for this project:\n\n\n\n\n[1]X. Zhang, J. Zhao, and Y. LeCun, \u2018\nCharacter-level convolutional networks for text classification\n\u2019, in Advances in neural information processing systems, 2015, pp. 649\u2013657.\n\n\n[2]T. Miyato, A. M. Dai, and I. Goodfellow, \u2018\nAdversarial Training Methods for Semi-Supervised Text Classification\n\u2019, arXiv preprint arXiv:1605.07725, 2016.\n\n\n[3]J. Collins, J. Sohl-Dickstein, and D. Sussillo, \u2018\nCapacity and Trainability in Recurrent Neural Networks\n\u2019, arXiv preprint arXiv:1611.09913, 2016.\n\n\n[4]Y. Kim, \u2018\nConvolutional neural networks for sentence classification\n\u2019, arXiv preprint arXiv:1408.5882, 2014.\n\n\n[5]A. Graves, \u2018\nGenerating sequences with recurrent neural networks\n\u2019, arXiv preprint arXiv:1308.0850, 2013.\n\n\n[6]A. Gatt and E. Khramer, \u2018\nSurvey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation\n\u2019, Knowledge-Based Systems, vol. 18, no. 4, pp. 235\u2013242, 2017.\n\n\n[6]W. Shang, K. Sohn, D. Almeida, and H. Lee, \u2018\nUnderstanding and improving convolutional neural networks via concatenated rectified linear units\n\u2019, in Proceedings of the International Conference on Machine Learning (ICML), 2016.\n\n\n[7]M. D. Zeiler and R. Fergus, \u2018\nVisualizing and understanding convolutional networks\n\u2019, in European conference on computer vision, 2014, pp. 818\u2013833.\n\n\n[8]I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, \u2018\nGenerative Adversarial Networks\n\u2018, arXiv preprint arXiv:1406.2661v1, 2014\n\n\n\n\n\n\nWebsites / Blogs\n\n\nThe following websites / blogs were used as references for this project:\n\n\n\n\n[1]\u2018Adventures in Narrated Reality \u2013 Artists and Machine Intelligence \u2013 Medium\u2019. [Online]. Available: \nhttps://medium.com/artists-and-machine-intelligence/adventures-in-narrated-reality-6516ff395ba3\n. [Accessed: 07-Apr-2017].\n\n\n[2]\u2018NaNoGenMo/2016\u2019, GitHub. [Online]. Available: \nhttps://github.com/NaNoGenMo/2016\n. [Accessed: 10-Apr-2017].\n\n\n[3]A. Karpathy, \u2018The Unreasonable Effectiveness of Recurrent Neural Networks\u2019. [Online]. Available: \nhttp://karpathy.github.io/2015/05/21/rnn-effectiveness/\n. [Accessed: 05-Apr-2017].\n\n\n[4]Fran\u00e7ois Chollet, \u2018How convolutional neural networks see the world\u2019. [Online]. Available: \nhttps://blog.keras.io/how-convolutional-neural-networks-see-the-world.html\n. [Accessed: 18-May-2017].\n\n\n\n\n\n\nDatasets and pre-trained models\n\n\nThe dataset we've used can be found as \nText files\n, \nConcatened text file\n or \nNumpy dumped array\n.\n\n\nThe best model we've found so far can be found \nhere\n.",
            "title": "Resources"
        },
        {
            "location": "/resources/#articles",
            "text": "The following articles were used as references for this project:   [1]X. Zhang, J. Zhao, and Y. LeCun, \u2018 Character-level convolutional networks for text classification \u2019, in Advances in neural information processing systems, 2015, pp. 649\u2013657.  [2]T. Miyato, A. M. Dai, and I. Goodfellow, \u2018 Adversarial Training Methods for Semi-Supervised Text Classification \u2019, arXiv preprint arXiv:1605.07725, 2016.  [3]J. Collins, J. Sohl-Dickstein, and D. Sussillo, \u2018 Capacity and Trainability in Recurrent Neural Networks \u2019, arXiv preprint arXiv:1611.09913, 2016.  [4]Y. Kim, \u2018 Convolutional neural networks for sentence classification \u2019, arXiv preprint arXiv:1408.5882, 2014.  [5]A. Graves, \u2018 Generating sequences with recurrent neural networks \u2019, arXiv preprint arXiv:1308.0850, 2013.  [6]A. Gatt and E. Khramer, \u2018 Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation \u2019, Knowledge-Based Systems, vol. 18, no. 4, pp. 235\u2013242, 2017.  [6]W. Shang, K. Sohn, D. Almeida, and H. Lee, \u2018 Understanding and improving convolutional neural networks via concatenated rectified linear units \u2019, in Proceedings of the International Conference on Machine Learning (ICML), 2016.  [7]M. D. Zeiler and R. Fergus, \u2018 Visualizing and understanding convolutional networks \u2019, in European conference on computer vision, 2014, pp. 818\u2013833.  [8]I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, \u2018 Generative Adversarial Networks \u2018, arXiv preprint arXiv:1406.2661v1, 2014",
            "title": "Articles"
        },
        {
            "location": "/resources/#websites-blogs",
            "text": "The following websites / blogs were used as references for this project:   [1]\u2018Adventures in Narrated Reality \u2013 Artists and Machine Intelligence \u2013 Medium\u2019. [Online]. Available:  https://medium.com/artists-and-machine-intelligence/adventures-in-narrated-reality-6516ff395ba3 . [Accessed: 07-Apr-2017].  [2]\u2018NaNoGenMo/2016\u2019, GitHub. [Online]. Available:  https://github.com/NaNoGenMo/2016 . [Accessed: 10-Apr-2017].  [3]A. Karpathy, \u2018The Unreasonable Effectiveness of Recurrent Neural Networks\u2019. [Online]. Available:  http://karpathy.github.io/2015/05/21/rnn-effectiveness/ . [Accessed: 05-Apr-2017].  [4]Fran\u00e7ois Chollet, \u2018How convolutional neural networks see the world\u2019. [Online]. Available:  https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html . [Accessed: 18-May-2017].",
            "title": "Websites / Blogs"
        },
        {
            "location": "/resources/#datasets-and-pre-trained-models",
            "text": "The dataset we've used can be found as  Text files ,  Concatened text file  or  Numpy dumped array .  The best model we've found so far can be found  here .",
            "title": "Datasets and pre-trained models"
        },
        {
            "location": "/about/",
            "text": "About\n\n\nContext\n\n\nThe Tiramisu Project is originally a 4th year student project of the \nESIEA\n. At the end of his year, one of the student decided to complete the project as a part of his internship in the \nARNUM\n laboratory.\n\n\n\n\nThe authors of these softwares are \nSt\u00e9phane Charavit\n, \nMaxime Sazadaly\n and \nCl\u00e9mence Viard\n.\n\n\nWhy 'Tiramisu' ?\n\n\nWell. Who doesn't like cakes? There's absolutely no reason. \n\n\nBut if you really insist, Tiramisu stands for \nT\next \nI\nnputs \nR\nead \nA\ns \nM\natrices for the \nI\nnterpretation of their \nS\nimilarity in \nU\nTF-8.\n\n\nLet's stick to the cake thing, okay ?\n\n\nContact\n\n\nIf you wish to contact the laboratory, feel free to follow \nthis link\n.\n\n\nIf you wish to contact the author of this project, feel free to comment on \nthis github\n.\n\n\nLicense\n\n\nThis is free and unencumbered software released into the public domain.\n\n\nAnyone is free to copy, modify, publish, use, compile, sell, or\ndistribute this software, either in source code form or as a compiled\nbinary, for any purpose, commercial or non-commercial, and by any\nmeans.\n\n\nIn jurisdictions that recognize copyright laws, the author or authors\nof this software dedicate any and all copyright interest in the\nsoftware to the public domain. We make this dedication for the benefit\nof the public at large and to the detriment of our heirs and\nsuccessors. We intend this dedication to be an overt act of\nrelinquishment in perpetuity of all present and future rights to this\nsoftware under copyright law.\n\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\nIN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR\nOTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\nARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\nOTHER DEALINGS IN THE SOFTWARE.\n\n\nFor more information, please refer to \nhttp://unlicense.org",
            "title": "About"
        },
        {
            "location": "/about/#about",
            "text": "",
            "title": "About"
        },
        {
            "location": "/about/#context",
            "text": "The Tiramisu Project is originally a 4th year student project of the  ESIEA . At the end of his year, one of the student decided to complete the project as a part of his internship in the  ARNUM  laboratory.   The authors of these softwares are  St\u00e9phane Charavit ,  Maxime Sazadaly  and  Cl\u00e9mence Viard .",
            "title": "Context"
        },
        {
            "location": "/about/#why-tiramisu",
            "text": "Well. Who doesn't like cakes? There's absolutely no reason.   But if you really insist, Tiramisu stands for  T ext  I nputs  R ead  A s  M atrices for the  I nterpretation of their  S imilarity in  U TF-8.  Let's stick to the cake thing, okay ?",
            "title": "Why 'Tiramisu' ?"
        },
        {
            "location": "/about/#contact",
            "text": "If you wish to contact the laboratory, feel free to follow  this link .  If you wish to contact the author of this project, feel free to comment on  this github .",
            "title": "Contact"
        },
        {
            "location": "/about/#license",
            "text": "This is free and unencumbered software released into the public domain.  Anyone is free to copy, modify, publish, use, compile, sell, or\ndistribute this software, either in source code form or as a compiled\nbinary, for any purpose, commercial or non-commercial, and by any\nmeans.  In jurisdictions that recognize copyright laws, the author or authors\nof this software dedicate any and all copyright interest in the\nsoftware to the public domain. We make this dedication for the benefit\nof the public at large and to the detriment of our heirs and\nsuccessors. We intend this dedication to be an overt act of\nrelinquishment in perpetuity of all present and future rights to this\nsoftware under copyright law.  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\nIN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR\nOTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\nARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\nOTHER DEALINGS IN THE SOFTWARE.  For more information, please refer to  http://unlicense.org",
            "title": "License"
        },
        {
            "location": "/faq/",
            "text": "",
            "title": "FAQ"
        }
    ]
}